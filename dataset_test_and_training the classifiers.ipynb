{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and  sorting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    (folder_name,int(re.findall(r\"\\d\\d\\d\\d\",file_name)[0])): \"lfw/%s/%s\" % (folder_name,file_name)\n",
    "    for folder_name in os.listdir(\"dataset/lfw/\")\n",
    "    for file_name in os.listdir(\"dataset/lfw/\"+folder_name)\n",
    "}\n",
    "#dataset[('Mehdi_Ghanimifard','1')] = 'dataset/lfw/Mehdi_Ghanimifard/Mehdi_Ghanimifard_0001.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dataset/embeddings/embeddings_align', 'rb') as f:\n",
    "    embeddings = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dataset/embeddings/imgpaths', 'rb') as f:\n",
    "    imgpaths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating embeddings and their difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "a = []\n",
    "b = []\n",
    "for index, path in enumerate(imgpaths):\n",
    "    #print(path, emb[index])\n",
    "    if 'Abdullah_al-Attiyah' in path:\n",
    "        a.append(embeddings[index])\n",
    "        img = cv2.cvtColor(cv2.imread(\"dataset/\"+path,), cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    if 'Nicole_Kidman' in path:\n",
    "        b.append(embeddings[index])\n",
    "        img = cv2.cvtColor(cv2.imread(\"dataset/\"+path,), cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(a), len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(a[0], a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(b[0], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ai in a:\n",
    "    np.dot(ai, b[0]),\n",
    "    np.dot(a[1], b[0]),\n",
    "    np.dot(a[2], b[0]),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the embeddings without alignment (They don't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dataset/embeddings/embeddings', 'rb') as f:\n",
    "    embeddings_tricky = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_tricky = []\n",
    "b_tricky = []\n",
    "for index, path in enumerate(imgpaths):\n",
    "    #print(path, emb[index])\n",
    "    if 'Abdullah_al-Attiyah' in path:\n",
    "        a_tricky.append(embeddings_tricky[index])\n",
    "        img = cv2.cvtColor(cv2.imread(\"dataset/\"+path,), cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    if 'Nicole_Kidman' in path:\n",
    "        b_tricky.append(embeddings_tricky[index])\n",
    "        img = cv2.cvtColor(cv2.imread(\"dataset/\"+path,), cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(a_tricky[0], a_tricky[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(np.abs(a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# understand this - understood!\n",
    "for (name, num), address in dataset.items():\n",
    "    print((name, num),address)\n",
    "    print(imgpaths.index(address))\n",
    "    print(embeddings[num])\n",
    "    break\n",
    "\n",
    "# reading attributes\n",
    "dataset_att = dict()\n",
    "with open('dataset/lfw_attributes.txt') as att_file:\n",
    "    for index, line in enumerate(att_file):\n",
    "        items = line.split('\\t')\n",
    "        \n",
    "        if index == 1:\n",
    "            attributes = items[3:]\n",
    "        if index > 1:\n",
    "            dataset_att[dataset[(items[0].replace(' ', '_'), int(items[1]))]] = np.array(items[2:]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(dataset_att.items())[0], len(list(dataset_att.items())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing faces without embeddings or attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dataset_att), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dataset/embeddings/skippedimg.txt\") as f:\n",
    "    skippedimg = [\"lfw\"+l.strip(\"\\n\")[9:] for l in f]\n",
    "\n",
    "#lfw/Abdoulaye_Wade/Abdoulaye_Wade_0003.jpg     \n",
    "\n",
    "#If we want to be able to associate image with embedding we need to know which image we use.\n",
    "usedimg = [address for _,address in dataset.items() if address in dataset_att and address not in skippedimg]\n",
    "#print(skippedimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features for all classifiers: ~13000 data points\n",
    "# for each file you have a vector of features:\n",
    "\n",
    "# fix this: there are some datapoints missing\n",
    "# Mehdi: I just wrote a fix for missing images in attribute dataset. There is still a minor work left here:\n",
    "# Anna : Fixed this\n",
    "\n",
    "addresss =[]\n",
    "for _,address in dataset.items():\n",
    "    if address in skippedimg:\n",
    "        addresss.append(address)\n",
    "        #print(address)\n",
    "a = sorted(addresss)\n",
    "#print(a)\n",
    "print(len(embeddings))\n",
    "X = [\n",
    "    embeddings[imgpaths.index(address)]\n",
    "    for _, address in dataset.items()\n",
    "    if address in dataset_att and address not in skippedimg\n",
    "]\n",
    "\n",
    "# 73 classifiers (each have their own y)\n",
    "# for each classifier (attribute):\n",
    "# for each file:\n",
    "#  either {-1, 1}\n",
    "Y = [\n",
    "    [\n",
    "        1 if dataset_att[address][i] > 0 else -1\n",
    "        for _, address in dataset.items()\n",
    "        if address in dataset_att and address not in skippedimg \n",
    "    ]\n",
    "    for i, att_name in enumerate(attributes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X), len(Y[0])\n",
    "print(X[0])\n",
    "print(Y[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make train, test, dev split\n",
    "split = int(len(X)*0.8)\n",
    "X_train = X[:split]\n",
    "X_2 = X[split:]\n",
    "nsplit = int(len(X_2)*0.5)\n",
    "X_dev = X_2[:nsplit]\n",
    "X_test = X_2[nsplit:]\n",
    "\n",
    "print(len(X_train),len(X_dev),len(X_test))\n",
    "\n",
    "Y_train=[]\n",
    "Y_dev=[]\n",
    "Y_test=[]\n",
    "\n",
    "for attr in Y:\n",
    "    Y_train.append(attr[:split])\n",
    "    Y_2 = attr[split:]\n",
    "    Y_dev.append(Y_2[:nsplit])\n",
    "    Y_test.append(Y_2[nsplit:])\n",
    "      \n",
    "    \n",
    "print(len(Y_train[0]),len(Y_dev[0]),len(Y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# for each classifier y, write a svm model\n",
    "# fit parameters of the model based on (X, y)\n",
    "\n",
    "classifier_list=[]\n",
    "for i,attribute in enumerate(Y_train):\n",
    "    classifier = svm.LinearSVC(max_iter=2000)\n",
    "    classifier.fit(X_train,Y_train[i])\n",
    "    classifier_list.append(classifier)\n",
    "\n",
    "pickle.dump(classifier_list,open(\"classifiers\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"classifiers\", 'rb') as f:\n",
    "    classifier_list = pickle.load(f)\n",
    "s=\"\"\n",
    "for i,classifier in enumerate(classifier_list):\n",
    "    clf = classifier\n",
    "    y_guess =clf.predict(X_test)\n",
    "    #y_guess = (y_guess+1)/2\n",
    "    #print(type(Y_test[i]))\n",
    "    #y_true = [ for result in Y_test[i]]\n",
    "    #classes=[\"not\"+attributes[i],attributes[i]]\n",
    "    #print(classification_report(y_true,y_guess,classes))\n",
    "    #print(\"Attribute: \"+attributes[i])\n",
    "  \n",
    "    print(\"Accuracy:\"+str(round(accuracy_score(Y_test[i], y_guess)*100,2))+\"\\%\")\n",
    "    \n",
    "    print(\"F-score:\"+str(round(f1_score(Y_test[i], y_guess,)*100,2))+\"%\")\n",
    "  \n",
    "    print(\"Precision:\"+str(round(precision_score(Y_test[i], y_guess)*100,2))+\"%\")\n",
    "\n",
    "    print(\"Recall:\"+str(round(recall_score(Y_test[i], y_guess)*100,2))+\"%\")\n",
    "   \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mehdi's classification (this is not used in the game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_total = np.array(X)\n",
    "y_total = np.array(Y).T\n",
    "\n",
    "# shuffle two list together:\n",
    "shuffle_indices = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "x_total_1 = x_total[shuffle_indices, :]\n",
    "y_total_1 = y_total[shuffle_indices, :]\n",
    "\n",
    "# separate test and train\n",
    "x_test, x_train = np.split(x_total, [int(x_total.shape[0]*.1)])\n",
    "y_test, y_train = np.split(y_total, [int(y_total.shape[0]*.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test.shape, x_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "feature_size = 128\n",
    "attribute_size = len(attributes)\n",
    "\n",
    "face2embeddings = tf.placeholder(dtype=tf.float32, shape=[None, feature_size])\n",
    "face2attributes = tf.placeholder(dtype=tf.float32, shape=[None, attribute_size])\n",
    "\n",
    "# MultiLayer Perceptrone (MLP):\n",
    "\n",
    "# layer one:\n",
    "hidden_dim_1 = features*2\n",
    "weight_1 = tf.Variable(tf.truncated_normal(shape=[feature_size, hidden_dim_1]))\n",
    "bias_1 = tf.Variable(tf.constant(0.1, shape=[hidden_dim_1]))\n",
    "layer_1 = tf.nn.tanh(tf.add(tf.matmul(face2embeddings, weight_1), bias_1))\n",
    "\n",
    "# layer two:\n",
    "hidden_dim_2 = features\n",
    "weight_2 = tf.Variable(tf.truncated_normal(shape=[hidden_dim_1, hidden_dim_2]))\n",
    "bias_2 = tf.Variable(tf.constant(0.1, shape=[hidden_dim_2]))\n",
    "layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weight_2), bias_2))\n",
    "\n",
    "# finial layer:\n",
    "weight_final = tf.Variable(tf.truncated_normal(shape=[hidden_dim_2, attribute_size]))\n",
    "bias_final = tf.Variable(tf.constant(0.1, shape=[attribute_size]))\n",
    "layer_final = tf.nn.tanh(tf.add(tf.matmul(layer_2, weight_final), bias_final))\n",
    "\n",
    "\n",
    "losses = tf.reduce_sum((face2attributes - layer_final)**2, axis=0)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "training_step = tf.train.AdamOptimizer().minimize(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 2000\n",
    "\n",
    "for e in range(epoch):\n",
    "    train_loss, _ = sess.run([loss, training_step], {face2embeddings: x_train, face2attributes: y_train})\n",
    "    test_loss = sess.run(loss, {face2embeddings: x_test, face2attributes: y_test})\n",
    "    \n",
    "    print(\"epoch:\", e, \"train loss:\", train_loss, \"test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'mlp-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = sess.run(layer_final, {face2embeddings: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sign(np.array([1,-1])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_results = np.sign(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_file = open(\"resultsMLP2.txt\", \"w\")\n",
    "for i, attribute in enumerate(attributes):\n",
    "    y_pred = np.sign(final_results.T[i]+1)\n",
    "    y_true = np.sign(y_test.T[i]+1)\n",
    "    classes = ['not %s' % attribute, attribute]\n",
    "    report_file.write(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The beginning of the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parser idea:\n",
    "sample = \"it isn't a man with no glasses\"\n",
    "\n",
    "# tockenization, normalization\n",
    "sample = sample.lower()\n",
    "sample = sample.replace('hasn\\'t', 'has not').replace('isn\\'t', 'is not')\n",
    "sample = sample.replace(' a ', ' ').replace(' an ', ' ').replace(' the ', ' ')\n",
    "sample = sample.replace('not ', 'not_').replace('no ', 'no_')\n",
    "sample_tockenized = sample.split()\n",
    "print(sample_tockenized)\n",
    "\n",
    "# dictionary\n",
    "#print(', '.join(attributes))\n",
    "attributes_normal = [a.lower().replace() for a in attributes]\n",
    "\n",
    "att_synonym = {a.lower(): [a.lower()] for a in attributes}\n",
    "att_antonym = {a.lower(): ['not_'+a.lower(), 'no_'+a.lower()] for a in attributes}\n",
    "# some manual lists \n",
    "att_synonym['male'] += ['man']\n",
    "att_antonym['male'] += ['not_man', 'no_man']\n",
    "print(att_synonym)\n",
    "\n",
    "# parser\n",
    "for attribute in attributes:\n",
    "    if len([1 for a in att_synonym[attribute.lower()] if a in sample_tockenized]):\n",
    "        print(\"the sample has/is\", attribute)\n",
    "    if len([1 for a in att_antonym[attribute.lower()] if a in sample_tockenized]):\n",
    "        print(\"the sample has/is not\", attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The beginning of the update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parser_imp import parser\n",
    "import random\n",
    "\n",
    "datasetlist = list(dataset.items())\n",
    "facenr = random.randint(0,len(datasetlist))\n",
    "\n",
    "imgpath = dataset[datasetlist[facenr][0]]\n",
    "\n",
    "#embedd = embeddings[imgpaths.index(imgpath)] \n",
    "\n",
    "print(datasetlist[facenr][0])\n",
    "print(imgpath)\n",
    "\n",
    "print(dataset_att[imgpath])\n",
    "#ownpath=\"dataset/lfw/Aaron_Pena/Aaron_Pena_0001.jpg\"\n",
    "\n",
    "#embedd = embeddings[imgpaths.index(ownpath)]\n",
    "\n",
    "#for (name, num), address in dataset.items():\n",
    "    #print((name, num),address)\n",
    "    #print(imgpaths.index(address))\n",
    "    #print(embeddings[num])\n",
    "    #break\n",
    "\n",
    "\n",
    "#with open(\"classifiers\", 'rb') as f:\n",
    "    #classifier_list = pickle.load(f)\n",
    "    \n",
    "with open(\"classifiersMLP\", 'rb') as f:\n",
    "    classifier_list = pickle.load(f)\n",
    "\n",
    "result = []\n",
    "for i,classifier in enumerate(classifier_list):\n",
    "    clf = classifier\n",
    "    y_guess =clf.predict(embedd)\n",
    "    result.append((attributes[i],y_guess))\n",
    "    print(attributes[i])\n",
    "    print(y_guess)\n",
    "    \n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
